# ğŸ¤– AI Knowledge Assistant

A full-stack AI-powered knowledge assistant that combines RAG (Retrieval-Augmented Generation), web search, and conversational AI into a unified interface. Upload documents, ask questions, and get intelligent responses with source citations.

![AI Knowledge Assistant](./docs/screenshots/main-interface.png)

## âœ¨ Features

- **ğŸ“„ Document Intelligence** - Upload PDFs, DOCX, TXT, and Markdown files. The system chunks, embeds, and indexes your documents for semantic search.

- **ğŸ” Smart Query Routing** - AI automatically determines whether to search your documents, the web, or use its general knowledge based on your question.

- **ğŸŒ Web Search Integration** - Real-time web search via Tavily API for current information and news.

- **ğŸ§® Built-in Calculator** - Mathematical calculations handled automatically when detected.

- **ğŸ’¬ Conversation Memory** - Maintains context across your chat session for natural follow-up questions.

- **ğŸ“š Source Citations** - Every answer includes citations linking back to the source documents or web pages.

- **ğŸŒ™ Dark Mode** - Beautiful dark theme UI with smooth transitions.

## ğŸ› ï¸ Tech Stack

### Backend
- **FastAPI** - High-performance Python web framework
- **LangChain** - LLM orchestration and RAG pipeline
- **OpenAI GPT-4o** - Large language model for responses
- **Pinecone** - Vector database for document embeddings
- **Tavily** - Web search API

### Frontend
- **Next.js 16** - React framework with App Router
- **TypeScript** - Type-safe development
- **Tailwind CSS** - Utility-first styling
- **Lucide React** - Beautiful icons

### Infrastructure
- **Railway** - Backend hosting
- **Vercel** - Frontend hosting
- **GitHub** - Version control & CI/CD

## ğŸš€ Live Demo

- **Frontend:** [ai-knowledge-assistant.vercel.app](https://ai-knowledge-assistant-git-main-eric-bolanders-projects.vercel.app)
- **Backend API:** [ai-knowledge-assistant-production.up.railway.app](https://ai-knowledge-assistant-production-7cdf.up.railway.app/health)

## ğŸ“¸ Screenshots

### Main Interface
Clean, intuitive chat interface with example queries for documents, web search, calculations, and more.

![Main Interface](./docs/screenshots/main-interface.png)

### Document Upload & Processing
Drag-and-drop file upload with real-time chunking feedback. Supports PDF, DOCX, TXT, and Markdown files.

![Document Upload](./docs/screenshots/document-upload2.png)

### Knowledge Base Sidebar
View indexed vectors, uploaded documents, and manage your knowledge base.

![Sidebar](./docs/screenshots/SideBar.png)

### Document RAG with Citations
Ask questions about your uploaded documents and get answers with source citations showing relevance scores.

![Document RAG](./docs/screenshots/Document-RAG.png)

### Web Search Integration
Real-time web search for current events and information not in your documents, with clickable source links.

![Web Search](./docs/screenshots/Web-Search.png)

### Multi-Turn Conversations
Conversation memory enables natural follow-up questions - the AI remembers context from previous messages.

![Multi-Turn Conversation](./docs/screenshots/Multi-Turn.png)

## ğŸ“¦ Installation

### Prerequisites
- Python 3.11+
- Node.js 18+
- API Keys for: OpenAI, Pinecone, Tavily

### Backend Setup

```bash
cd backend

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: .\venv\Scripts\Activate

# Install dependencies
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env with your API keys

# Run the server
uvicorn main:app --reload
```

### Frontend Setup

```bash
cd frontend

# Install dependencies
npm install

# Configure environment
cp .env.example .env.local
# Edit .env.local with your backend URL

# Run development server
npm run dev
```

## ğŸ”§ Environment Variables

### Backend (.env)
```
OPENAI_API_KEY=your-openai-api-key
PINECONE_API_KEY=your-pinecone-api-key
TAVILY_API_KEY=your-tavily-api-key
PINECONE_INDEX_NAME=knowledge-assistant
```

### Frontend (.env.local)
```
NEXT_PUBLIC_API_URL=http://localhost:8000
```

## ğŸ“ Project Structure

```
ai-knowledge-assistant/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ agents.py      # Multi-tool AI agent
â”‚   â”‚   â”œâ”€â”€ citations.py   # Source citation manager
â”‚   â”‚   â”œâ”€â”€ config.py      # Configuration settings
â”‚   â”‚   â”œâ”€â”€ document.py    # Document processing
â”‚   â”‚   â”œâ”€â”€ embeddings.py  # Vector embeddings
â”‚   â”‚   â””â”€â”€ retrieval.py   # RAG retrieval logic
â”‚   â”œâ”€â”€ main.py            # FastAPI application
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app/           # Next.js app router
â”‚   â”‚   â”œâ”€â”€ components/    # React components
â”‚   â”‚   â”œâ”€â”€ lib/           # API client
â”‚   â”‚   â””â”€â”€ types/         # TypeScript types
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ tailwind.config.ts
â”‚
â”œâ”€â”€ docs/
â”‚   â””â”€â”€ screenshots/       # Application screenshots
â”‚
â””â”€â”€ README.md
```

## ğŸ”Œ API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/health` | Health check with index stats |
| POST | `/chat` | Send a message, get AI response |
| POST | `/upload` | Upload a document |
| GET | `/documents` | List uploaded documents |
| DELETE | `/documents/{filename}` | Delete a document |
| POST | `/clear-history` | Clear conversation history |
| POST | `/index/clear` | Clear all vectors from index |

## ğŸ¯ How It Works

1. **Document Upload** - Documents are processed, split into chunks, and embedded using OpenAI's text-embedding-3-small model.

2. **Query Classification** - When you ask a question, the AI agent classifies the intent (document search, web search, calculator, or general knowledge).

3. **Tool Execution** - Based on classification:
   - **Documents**: Semantic search against your uploaded files
   - **Web Search**: Real-time Tavily search for current info
   - **Calculator**: Safe math expression evaluation
   - **General**: Direct LLM response

4. **Response Generation** - GPT-4o generates a response using the retrieved context, with proper source citations.

## ğŸ§ª Testing

```bash
# Backend tests
cd backend
pytest

# Test API endpoints
curl http://localhost:8000/health
```

## ğŸš¢ Deployment

### Backend (Railway)
1. Connect your GitHub repo to Railway
2. Set root directory to `backend`
3. Add environment variables
4. Deploy!

### Frontend (Vercel)
1. Import project from GitHub
2. Set root directory to `frontend`
3. Add `NEXT_PUBLIC_API_URL` environment variable
4. Deploy!

## ğŸ›£ï¸ Roadmap

- [ ] Multi-user authentication
- [ ] Chat history persistence
- [ ] More file format support (CSV, Excel)
- [ ] Voice input/output
- [ ] Custom model selection

## ğŸ‘¨â€ğŸ’» Author

**Eric Bolander**
- GitHub: [@Ebolander23](https://github.com/Ebolander23)
- LinkedIn: [Eric Bolander](https://www.linkedin.com/in/eric-bolander/)

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

Built with â¤ï¸ as a learning project to explore modern AI application architecture.
